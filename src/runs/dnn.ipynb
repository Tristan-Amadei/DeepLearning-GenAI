{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from dnn import DNN\n",
    "from dbn import DBN\n",
    "from load_data import *\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_alpha = lire_alpha_digits(['A', 'E', 'X', '4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = load_mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "batch_size = 512\n",
    "learning_rate = 1e-3\n",
    "patience = 10\n",
    "\n",
    "nb_layers = 3\n",
    "neurons = [512, 256, 256]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alpha Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbn_alpha = DBN(X=binary_alpha, L=nb_layers, qs=neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbn_alpha.train_DBN(epochs=epochs, learning_rate=learning_rate, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 4000x1200 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "alpha_gen = dbn_alpha.generer_image_DBN(num_samples=30, gibbs_steps=2000, image_size=(20, 16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dnn(pretrain, nb_layers, neurons, suptitle, X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test, \n",
    "            epochs=epochs, learning_rate=learning_rate, batch_size=batch_size, plot_=True):\n",
    "    dnn = DNN(X_train, y_train, num_classes=10, num_hidden_layers=nb_layers, neurons=neurons,\n",
    "                X_val=X_test, y_val=y_test)\n",
    "    if pretrain:\n",
    "        dnn.pretrain_DNN(epochs=epochs, learning_rate=learning_rate, batch_size=batch_size)\n",
    "    losses, accuracies, val_losses, val_accuracies = dnn.retropropagation(\n",
    "    epochs=epochs, learning_rate=learning_rate, batch_size=batch_size, plot_=plot_, patience=patience, \n",
    "    suptitle=suptitle)\n",
    "    return dnn, losses, accuracies, val_losses, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_list(l, path):\n",
    "    with open(path, \"wb\") as fp: \n",
    "        pickle.dump(l, fp)\n",
    "        \n",
    "def open_list(path):\n",
    "    with open(path, \"rb\") as fp:   \n",
    "        l = pickle.load(fp)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dirs(digit_figure):\n",
    "    if not os.path.exists(f'figure{digit_figure}'):\n",
    "        os.mkdir(f'figure{digit_figure}')\n",
    "    if not os.path.exists(f'figure{digit_figure}/dnns'):\n",
    "        os.mkdir(f'figure{digit_figure}/dnns')\n",
    "    if not os.path.exists(f'.figure{digit_figure}/losses'):\n",
    "        os.mkdir(f'figure{digit_figure}/losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(i, losses_nb_layers, accuracies_nb_layers, val_losses_nb_layers, val_accuracies_nb_layers, ax, xlabel):\n",
    "    ax[0].plot(losses_nb_layers[i][0], label='Train set- random init')\n",
    "    ax[0].plot(losses_nb_layers[i][1], '--', label='Train set - pretrain')\n",
    "    ax[0].plot(val_losses_nb_layers[i][0], label='Valid set- random init')\n",
    "    ax[0].plot(val_losses_nb_layers[i][1], '--', label='Valid set - pretrain')\n",
    "    ax[0].set_xlabel(xlabel)\n",
    "    ax[0].grid('on')\n",
    "    ax[0].legend()\n",
    "    \n",
    "    ax[1].plot(accuracies_nb_layers[i][0], label='Train set- random init')\n",
    "    ax[1].plot(accuracies_nb_layers[i][1], '--', label='Train set - pretrain')\n",
    "    ax[1].plot(val_accuracies_nb_layers[i][0], label='Valid set- random init')\n",
    "    ax[1].plot(val_accuracies_nb_layers[i][1], '--', label='Valid set - pretrain')\n",
    "    ax[1].grid('on')\n",
    "    ax[1].legend()\n",
    "    \n",
    "    if i == 0:\n",
    "        ax[0].set_title('Loss')\n",
    "        ax[1].set_title('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_and_save(digit_figure, name, nb_layers, neurons, suptitle,\n",
    "                 X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test, epochs=epochs, learning_rate=learning_rate, batch_size=batch_size, plot_=True):\n",
    "    if not os.path.exists(f'figure{digit_figure}/dnns/dnn_{name}.pkl'):\n",
    "        dnn, losses, accuracies, val_losses, val_accuracies = run_dnn(False, nb_layers, neurons, suptitle=suptitle + ' - random init', X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test, \n",
    "                epochs=epochs, learning_rate=learning_rate, batch_size=batch_size, plot_=plot_)\n",
    "        clear_output(wait=True)\n",
    "        dnn.save_weights(f'figure{digit_figure}/dnns/dnn_{name}.pkl')\n",
    "        save_list(losses, f'figure{digit_figure}/losses/losses_{name}.pkl')\n",
    "        save_list(accuracies, f'figure{digit_figure}/losses/accuracies_{name}.pkl')\n",
    "        save_list(val_losses, f'figure{digit_figure}/losses/val_losses_{name}.pkl')\n",
    "        save_list(val_accuracies, f'figure{digit_figure}/losses/val_accuracies_{name}.pkl')\n",
    "\n",
    "    print('pretrain -', name)\n",
    "    if not os.path.exists(f'figure{digit_figure}/dnns/pretrain_dnn_{name}.pkl'):\n",
    "        dnn, losses_pretrain, accuracies_pretrain, val_losses_pretrain, val_accuracies_pretrain = run_dnn(True, nb_layers, neurons, suptitle=suptitle + '- pretrain', X_train=X_train, y_train=y_train, \n",
    "                    X_test=X_test, y_test=y_test, epochs=epochs, learning_rate=learning_rate, batch_size=batch_size, plot_=plot_)\n",
    "        clear_output(wait=True)\n",
    "        dnn.save_weights(f'figure{digit_figure}/dnns/pretrain_dnn_{name}.pkl')\n",
    "        save_list(losses_pretrain, f'figure{digit_figure}/losses/pretrain_losses_{name}.pkl')\n",
    "        save_list(accuracies_pretrain, f'figure{digit_figure}/losses/pretrain_accuracies_{name}.pkl')\n",
    "        save_list(val_losses_pretrain, f'figure{digit_figure}/losses/pretrain_val_losses_{name}.pkl')\n",
    "        save_list(val_accuracies_pretrain, f'figure{digit_figure}/losses/pretrain_val_accuracies_{name}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [2, 3, 5, 7, 10]\n",
    "try:\n",
    "    create_dirs(1)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrain - 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: error = 0.1295\n",
      "Epoch 20: error = 0.0443\n",
      "Epoch 40: error = 0.0391\n",
      "Epoch 60: error = 0.037\n",
      "Epoch 80: error = 0.0356\n",
      "Epoch 100: error = 0.0345\n",
      "Epoch 120: error = 0.0338\n",
      "Epoch 140: error = 0.0332\n",
      "Epoch 160: error = 0.0327\n",
      "Epoch 180: error = 0.0323\n",
      "Epoch 200: error = 0.032\n",
      "Epoch 220: error = 0.0316\n",
      "Epoch 240: error = 0.0314\n",
      "Epoch 260: error = 0.0312\n",
      "Epoch 280: error = 0.031\n",
      "Epoch 300: error = 0.0309\n",
      "Epoch 320: error = 0.0307\n",
      "Epoch 340: error = 0.0306\n",
      "Epoch 360: error = 0.0305\n",
      "Epoch 380: error = 0.0304\n",
      "Epoch 400: error = 0.0303\n",
      "Epoch 420: error = 0.0302\n",
      "Epoch 440: error = 0.0301\n",
      "Epoch 460: error = 0.03\n",
      "Epoch 480: error = 0.0299\n",
      "Epoch 500: error = 0.0298\n",
      "Epoch 520: error = 0.0298\n",
      "Epoch 540: error = 0.0297\n",
      "Epoch 560: error = 0.0297\n",
      "Epoch 580: error = 0.0296\n",
      "Epoch 600: error = 0.0296\n",
      "Epoch 620: error = 0.0295\n",
      "Epoch 640: error = 0.0294\n",
      "Epoch 660: error = 0.0294\n",
      "Epoch 680: error = 0.0293\n",
      "Epoch 700: error = 0.0293\n",
      "Epoch 720: error = 0.0293\n",
      "Epoch 740: error = 0.0292\n",
      "Epoch 760: error = 0.0292\n",
      "Epoch 780: error = 0.0292\n",
      "Epoch 800: error = 0.0291\n",
      "Epoch 820: error = 0.0291\n",
      "Epoch 840: error = 0.029\n",
      "Epoch 860: error = 0.0289\n",
      "Epoch 880: error = 0.0289\n",
      "Epoch 900: error = 0.0289\n",
      "Epoch 920: error = 0.0288\n",
      "Epoch 940: error = 0.0288\n",
      "Epoch 960: error = 0.0287\n",
      "Epoch 980: error = 0.0287\n",
      "Epoch 999: error = 0.0287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [1:34:00<14:06:02, 5640.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: error = 0.2924\n",
      "Epoch 20: error = 0.127\n",
      "Epoch 40: error = 0.1128\n",
      "Epoch 60: error = 0.1057\n",
      "Epoch 80: error = 0.1019\n",
      "Epoch 100: error = 0.0994\n",
      "Epoch 120: error = 0.0978\n",
      "Epoch 140: error = 0.0965\n",
      "Epoch 160: error = 0.0957\n",
      "Epoch 180: error = 0.0949\n",
      "Epoch 200: error = 0.0943\n",
      "Epoch 220: error = 0.0939\n",
      "Epoch 240: error = 0.0933\n",
      "Epoch 260: error = 0.0929\n",
      "Epoch 280: error = 0.0926\n",
      "Epoch 300: error = 0.0922\n",
      "Epoch 320: error = 0.0918\n",
      "Epoch 340: error = 0.0914\n",
      "Epoch 360: error = 0.0911\n",
      "Epoch 380: error = 0.0909\n",
      "Epoch 400: error = 0.0905\n",
      "Epoch 420: error = 0.0903\n",
      "Epoch 440: error = 0.09\n",
      "Epoch 460: error = 0.0898\n",
      "Epoch 480: error = 0.0895\n",
      "Epoch 500: error = 0.0895\n",
      "Epoch 520: error = 0.0893\n",
      "Epoch 540: error = 0.0893\n",
      "Epoch 560: error = 0.089\n",
      "Epoch 580: error = 0.0888\n",
      "Epoch 600: error = 0.0886\n",
      "Epoch 620: error = 0.0886\n",
      "Epoch 640: error = 0.0884\n",
      "Epoch 660: error = 0.0883\n",
      "Epoch 680: error = 0.0881\n",
      "Epoch 700: error = 0.0881\n",
      "Epoch 720: error = 0.0881\n",
      "Epoch 740: error = 0.088\n",
      "Epoch 760: error = 0.088\n",
      "Epoch 780: error = 0.0879\n",
      "Epoch 800: error = 0.0876\n",
      "Epoch 820: error = 0.0876\n",
      "Epoch 840: error = 0.0876\n",
      "Epoch 860: error = 0.0876\n",
      "Epoch 880: error = 0.0875\n",
      "Epoch 900: error = 0.0874\n",
      "Epoch 920: error = 0.0873\n",
      "Epoch 940: error = 0.0873\n",
      "Epoch 960: error = 0.0871\n",
      "Epoch 980: error = 0.0869\n",
      "Epoch 999: error = 0.0868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [1:58:40<7:05:44, 3193.03s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: error = 0.1466\n",
      "Epoch 20: error = 0.0602\n",
      "Epoch 40: error = 0.0561\n",
      "Epoch 60: error = 0.0543\n",
      "Epoch 80: error = 0.0533\n",
      "Epoch 100: error = 0.0526\n",
      "Epoch 120: error = 0.0524\n",
      "Epoch 140: error = 0.0522\n",
      "Epoch 160: error = 0.0518\n",
      "Epoch 180: error = 0.0516\n",
      "Epoch 200: error = 0.0516\n",
      "Epoch 220: error = 0.0515\n",
      "Epoch 240: error = 0.0512\n",
      "Epoch 260: error = 0.0512\n",
      "Epoch 280: error = 0.0512\n",
      "Epoch 300: error = 0.051\n",
      "Epoch 320: error = 0.051\n",
      "Epoch 340: error = 0.0509\n",
      "Epoch 360: error = 0.0509\n",
      "Epoch 380: error = 0.0509\n",
      "Epoch 400: error = 0.0507\n",
      "Epoch 420: error = 0.0507\n",
      "Epoch 440: error = 0.0507\n",
      "Epoch 460: error = 0.0507\n",
      "Epoch 480: error = 0.0506\n",
      "Epoch 500: error = 0.0505\n",
      "Epoch 520: error = 0.0505\n"
     ]
    }
   ],
   "source": [
    "for nb_layer in layers:\n",
    "    print(f'Nb layer: {nb_layer}')\n",
    "    neurons = [200]*nb_layer\n",
    "    run_and_save(1, nb_layer, nb_layer, neurons, suptitle=f'{nb_layers} layers',\n",
    "                 X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test, epochs=epochs, learning_rate=learning_rate, batch_size=batch_size, plot_=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_nb_layers, accuracies_nb_layers, val_losses_nb_layers, val_accuracies_nb_layers = [], [], [], []\n",
    "for nb_layer in layers:\n",
    "    losses = open_list(f'figure1/losses/losses_{nb_layer}.pkl')\n",
    "    accuracies = open_list(f'figure1/losses/accuracies_{nb_layer}.pkl')\n",
    "    val_losses = open_list(f'figure1/losses/val_losses_{nb_layer}.pkl')\n",
    "    val_accuracies = open_list(f'figure1/losses/val_accuracies_{nb_layer}.pkl')\n",
    "    \n",
    "    losses_pretrain = open_list(f'figure1/losses/pretrain_losses_{nb_layer}.pkl')\n",
    "    accuracies_pretrain = open_list(f'figure1/losses/pretrain_accuracies_{nb_layer}.pkl')\n",
    "    val_losses_pretrain = open_list(f'figure1/losses/pretrain_val_losses_{nb_layer}.pkl')\n",
    "    val_accuracies_pretrain = open_list(f'figure1/losses/pretrain_val_accuracies_{nb_layer}.pkl')\n",
    "    \n",
    "    losses_nb_layers.append((losses, losses_pretrain))\n",
    "    accuracies_nb_layers.append((accuracies, accuracies_pretrain))\n",
    "    val_losses_nb_layers.append((val_losses, val_losses_pretrain))\n",
    "    val_accuracies_nb_layers.append((val_accuracies, val_accuracies_pretrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(len(layers), 2, figsize=(10, 4*len(layers)))\n",
    "for i in range(len(layers)):\n",
    "    plot(i, losses_nb_layers, accuracies_nb_layers, val_losses_nb_layers, val_accuracies_nb_layers, ax=axs[i], \n",
    "         xlabel=f'{layers[i]} layers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_neurons = [100, 200, 700, 1000]\n",
    "try:\n",
    "    create_dirs(2)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nb_neuron in nb_neurons:\n",
    "    neurons = [nb_neuron]*2\n",
    "    print(f'Nb neurons: {nb_neuron}')\n",
    "    run_and_save(2, nb_neuron, 2, neurons, suptitle=f'{nb_neuron} neurons',\n",
    "                 X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test, epochs=epochs, learning_rate=learning_rate, batch_size=batch_size, plot_=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_nb_neurons, accuracies_nb_neurons, val_losses_nb_neurons, val_accuracies_nb_neurons = [], [], [], []\n",
    "for nb_neuron in nb_neurons:\n",
    "    losses = open_list(f'figure2/losses/losses_{nb_neuron}.pkl')\n",
    "    accuracies = open_list(f'figure2/losses/accuracies_{nb_neuron}.pkl')\n",
    "    val_losses = open_list(f'figure2/losses/val_losses_{nb_neuron}.pkl')\n",
    "    val_accuracies = open_list(f'figure2/losses/val_accuracies_{nb_neuron}.pkl')\n",
    "    \n",
    "    losses_pretrain = open_list(f'figure2/losses/pretrain_losses_{nb_neuron}.pkl')\n",
    "    accuracies_pretrain = open_list(f'figure2/losses/pretrain_accuracies_{nb_neuron}.pkl')\n",
    "    val_losses_pretrain = open_list(f'figure2/losses/pretrain_val_losses_{nb_neuron}.pkl')\n",
    "    val_accuracies_pretrain = open_list(f'figure2/losses/pretrain_val_accuracies_{nb_neuron}.pkl')\n",
    "    \n",
    "    losses_nb_neurons.append((losses, losses_pretrain))\n",
    "    accuracies_nb_neurons.append((accuracies, accuracies_pretrain))\n",
    "    val_losses_nb_neurons.append((val_losses, val_losses_pretrain))\n",
    "    val_accuracies_nb_neurons.append((val_accuracies, val_accuracies_pretrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(len(nb_neurons), 2, figsize=(10, 4*len(nb_neurons)))\n",
    "for i in range(len(nb_neurons)):\n",
    "    plot(i, losses_nb_neurons, accuracies_nb_neurons, val_losses_nb_neurons, val_accuracies_nb_neurons, ax=axs[i], \n",
    "         xlabel=f'{nb_neurons[i]} neurons')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenTrains = [1000, 3000, 7000, 10_000, 30_000, 60_000]\n",
    "try:\n",
    "    create_dirs(3)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lenTrain in lenTrains:\n",
    "    print(f'Length train: {lenTrain}')\n",
    "    indices = np.random.choice(range(len(X_train)), size=lenTrain, replace=False)\n",
    "    sub_X_train = X_train[indices]\n",
    "    sub_y_train = y_train[indices]\n",
    "    \n",
    "    run_and_save(3, lenTrain, 2, [200, 200], suptitle=f'{lenTrain} training samples',\n",
    "                 X_train=sub_X_train, y_train=sub_y_train, X_test=X_test, y_test=y_test, epochs=epochs, learning_rate=learning_rate, batch_size=batch_size, plot_=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_nb_train, accuracies_nb_train, val_losses_nb_train, val_accuracies_nb_train = [], [], [], []\n",
    "for lenTrain in lenTrains:\n",
    "    losses = open_list(f'figure3/losses/losses_{lenTrain}.pkl')\n",
    "    accuracies = open_list(f'figure3/losses/accuracies_{lenTrain}.pkl')\n",
    "    val_losses = open_list(f'figure3/losses/val_losses_{lenTrain}.pkl')\n",
    "    val_accuracies = open_list(f'figure3/losses/val_accuracies_{lenTrain}.pkl')\n",
    "    \n",
    "    losses_pretrain = open_list(f'figure3/losses/pretrain_losses_{lenTrain}.pkl')\n",
    "    accuracies_pretrain = open_list(f'figure3/losses/pretrain_accuracies_{lenTrain}.pkl')\n",
    "    val_losses_pretrain = open_list(f'figure3/losses/pretrain_val_losses_{lenTrain}.pkl')\n",
    "    val_accuracies_pretrain = open_list(f'figure3/losses/pretrain_val_accuracies_{lenTrain}.pkl')\n",
    "    \n",
    "    losses_nb_train.append((losses, losses_pretrain))\n",
    "    accuracies_nb_train.append((accuracies, accuracies_pretrain))\n",
    "    val_losses_nb_train.append((val_losses, val_losses_pretrain))\n",
    "    val_accuracies_nb_train.append((val_accuracies, val_accuracies_pretrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(len(lenTrains), 2, figsize=(10, 4*len(lenTrains)))\n",
    "for i in range(len(lenTrains)):\n",
    "    plot(i, losses_nb_train, accuracies_nb_train, val_losses_nb_train, val_accuracies_nb_train, ax=axs[i], \n",
    "         xlabel=f'{lenTrains[i]} training samples')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
